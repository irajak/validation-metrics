{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (50 marks total)\n",
    "### Due: March 7 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (30 marks)\n",
    "\n",
    "### *Part 1A: Decision Function* \n",
    "\n",
    "Building on the first part of the previous assignment, we would like to test how changing the decision boundary for a linear model impacts the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignoring some deprication warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "Load spam data using the same method as assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33583c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "\n",
    "from yellowbrick.datasets import load_spam\n",
    "X, y = load_spam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696e475",
   "metadata": {},
   "source": [
    "Next, we need to split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split 10% of the data for the testing set (1 mark)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7a107",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing\n",
    "Based on assignment 2, follow the same data processing steps (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Data processing steps (if needed)\n",
    "# we don't need any additional processing\n",
    "# data is formatted fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (2 marks)\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "1. Instantiate model `LogisticRegression(max_iter=2000)`\n",
    "1. Split training data into training and validation sets (use 20% of the data for validation)\n",
    "1. Train the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split training data into training and validation sets and fit model to training data (2 marks)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_train, X_val, y_train_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train_train, y_train_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362d26f",
   "metadata": {},
   "source": [
    "### Step 4-5: Validate Model and Visualize Results (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3f305",
   "metadata": {},
   "source": [
    "Next, we can print the classification report and confusion matrix for this data set using the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print classification report (1 mark)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print confusion matrix (1 mark)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f305a",
   "metadata": {},
   "source": [
    "In this case, do we want to increase precision or recall? Based on your choice, select the appropriate direction to adjust the decision boundary. You can use either 1 or -1 as your new threshold, depending on your selected direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Predict values based on new decision function threshold and print classification report (1 mark)\n",
    "y_pred_new_threshold = (model.decision_function(X_val) > 1).astype(int)\n",
    "print(classification_report(y_val, y_pred_new_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print new confusion matrix (1 mark)\n",
    "cm_new = confusion_matrix(y_val, y_pred_new_threshold)\n",
    "sns.heatmap(cm_new, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efec22",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. Did you decide to move the threshold to increase recall or precision? Why?\n",
    "1. How did your decision impact the number of false negatives and false positives? What is a potential unintended consequence of changing the decision boundary in this context?\n",
    "1. Why did we use the validation data instead of the test data to check the new decision threshold?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683aaa15",
   "metadata": {},
   "source": [
    "### *Part 1B: Non-linear classification*\n",
    "\n",
    "Using the spam dataset from part 1A, compare the performance of two non-linear models to the linear model used in assignment 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac447c9c",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Models\n",
    "\n",
    "1. Import `LogisticRegression`, `SVC` and `RandomForestClassifier` from sklearn\n",
    "2. Instantiate models as `LogisticRegression(max_iter=2000)`, `SVC()` and `RandomForestClassifier(random_state=0, max_depth=10)`\n",
    "3. Implement the machine learning models using cross-validation (Step 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the training and validation accuracy for the three different models mentioned in Step 3. For this case, you can use `cross_validate()` with `cv=5` and `scoring='accuracy'` to get the training and validation data for each of the three models and calculate the accuracy results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5.1: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "1. Add the training accuracy and validation accuracy for each model to the `results` DataFrame\n",
    "1. Add the model names as the index for the DataFrame\n",
    "1. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=0, max_depth=10)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Training Accuracy\": cv_results['train_score'].mean(),\n",
    "        \"Validation Accuracy\": cv_results['test_score'].mean()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df258782",
   "metadata": {},
   "source": [
    "SVM is sensitive to feature ranges, so scaling may be needed. Look at the feature ranges and try using a scaling method to see if the SVM results are improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16524422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Look at the ranges for each feature (0.5 marks)\n",
    "# Hint: there is a built-in pandas function that you can use to view the statistics of your data\n",
    "print(X_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac209f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Implement scaling for SVM and print training and validation accuracies (1.5 marks)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(X_val_scaled)\n",
    "\n",
    "print(\"SVM Training Accuracy:\", svm_model.score(X_train_scaled, y_train))\n",
    "print(\"SVM Validation Accuracy:\", svm_model.score(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a21b4de",
   "metadata": {},
   "source": [
    "Which model gave us the best results? Use that model for the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13a54f",
   "metadata": {},
   "source": [
    "### Step 5.2: Visualize Classification Errors (3 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6022252f",
   "metadata": {},
   "source": [
    "In this section, print the classification report and confusion matrix to investigate the recall vs. precision for the best model. Use the full training set and testing set for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81931e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Train model and find predicted values for testing set using best model (1 mark)\n",
    "best_model = RandomForestClassifier(random_state=0, max_depth=10)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_test = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67439bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print classification report (1 mark)\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap (1 mark)\n",
    "cm_best = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cm_best, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. Which model did you select for part 5.2? How did it compare to the other models? \n",
    "1. Looking at the feature ranges, would a tree-based model or SVM make more sense for this dataset? Did using scaling for SVM improve the results? How did it perform compared to the random forest model?\n",
    "1. In your opinion, is it better to focus on changing the decision threshold or changing the model to improve precision/recall results? Why?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code for parts A and B. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 2: Regression (18 marks)\n",
    "\n",
    "For this section, we will be using the concrete example from yellowbrick. Since this dataset is highly non-linear, we will be evaluating how well different tree-based models work for this case.\n",
    "\n",
    "You will need to repeat the steps from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be imported using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1 mark)\n",
    "\n",
    "Check if there are any missing values and fill them in if necessary. Remove any non-numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Process the data - fill-in any missing values and remove any non-numeric columns (0.5 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df0b2a",
   "metadata": {},
   "source": [
    "The concrete data should already be split into the feature matrix and target vector. Inspect the first few columns of the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Inspect the first few rows of the feature matrix (0.5 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement and Validate Machine Learning Model (7 marks)\n",
    "\n",
    "1. Import any required libraries\n",
    "1. Split the data into training and testing sets (testing data should be 10% of the dataset)\n",
    "1. Train and validate the Decision Tree model with the training set (use `cross_validate()` with `cv=5` and `scoring='r2'`)\n",
    "    1. Test five different max_depth values: 3, 5, 7, 9 and 11\n",
    "1. Print the training and validation accuracy for the best max_depth results. Which max_depth gave us the best results?\n",
    "\n",
    "**Note**: for any random state parameters, you can use random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and testing sets (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cade259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test max_depths of 3, 5, 7, 9 and 11 for a decision tree model to find the best results (3 marks)\n",
    "# Hint: It is easier if you use a loop to evaluate each max_depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7dcbb1",
   "metadata": {},
   "source": [
    "Now that we have found the best results for a decision tree model with this dataset, let's compare this result to using `Random_Forest` or `GradientBoosting`. For both models, use `max_depth=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Calculate and display training and validation accuracies for both models using default hyperparameters (3 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786d1c8",
   "metadata": {},
   "source": [
    "Which model gave us the best results? Use that model for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Test Model (1 mark)\n",
    "\n",
    "Select the best model and calculate the testing accuracy using the R^2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Find test score using best model (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 3: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c484f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
